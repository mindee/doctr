<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-40DVRMX8T4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-40DVRMX8T4');
</script>
    <link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Preparing your model for inference" href="using_model_export.html" /><link rel="prev" title="Installation" href="installing.html" />

    <link rel="shortcut icon" href="_static/favicon.ico"/><meta name="generator" content="sphinx-5.0.2, furo 2022.06.21"/>
        <title>Choosing the right model - docTR documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=40978830699223671f4072448e654b5958f38b89" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/mindee.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f0f0f0;
  --color-code-foreground: black;
  --color-sidebar-background: #082747;
  --color-sidebar-background-border: #082747;
  --color-sidebar-caption-text: white;
  --color-sidebar-link-text--top-level: white;
  --color-sidebar-link-text: white;
  --sidebar-caption-font-size: normal;
  --color-sidebar-item-background--hover:  #5dade2;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-sidebar-background: #1a1c1e;
  --color-sidebar-background-border: #1a1c1e;
  --color-sidebar-caption-text: white;
  --color-sidebar-link-text--top-level: white;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-sidebar-background: #1a1c1e;
  --color-sidebar-background-border: #1a1c1e;
  --color-sidebar-caption-text: white;
  --color-sidebar-link-text--top-level: white;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">docTR documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="_static/Logo-docTR-white.png" alt="Logo"/>
  </div>
  
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Using DocTR</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Choosing the right model</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_model_export.html">Preparing your model for inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">doctr.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="io.html">doctr.io</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">doctr.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="transforms.html">doctr.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">doctr.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="choosing-the-right-model">
<h1>Choosing the right model<a class="headerlink" href="#choosing-the-right-model" title="Permalink to this heading">#</a></h1>
<p>The full Optical Character Recognition task can be seen as two consecutive tasks: text detection and text recognition.
Either performed at once or separately, to each task corresponds a type of deep learning architecture.</p>
<p>For a given task, DocTR provides a Predictor, which is composed of 2 components:</p>
<ul class="simple">
<li><p>PreProcessor: a module in charge of making inputs directly usable by the deep learning model.</p></li>
<li><p>Model: a deep learning model, implemented with all supported deep learning backends (TensorFlow &amp; PyTorch) along with its specific post-processor to make outputs structured and reusable.</p></li>
</ul>
<section id="text-detection">
<h2>Text Detection<a class="headerlink" href="#text-detection" title="Permalink to this heading">#</a></h2>
<p>The task consists of localizing textual elements in a given image.
While those text elements can represent many things, in DocTR, we will consider uninterrupted character sequences (words). Additionally, the localization can take several forms: from straight bounding boxes (delimited by the 2D coordinates of the top-left and bottom-right corner), to polygons, or binary segmentation (flagging which pixels belong to this element, and which donâ€™t).</p>
<section id="available-architectures">
<h3>Available architectures<a class="headerlink" href="#available-architectures" title="Permalink to this heading">#</a></h3>
<p>The following architectures are currently supported:</p>
<ul class="simple">
<li><p><a class="reference external" href="models.html#doctr.models.detection.linknet16">linknet16</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.detection.db_resnet50">db_resnet50</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.detection.db_mobilenet_v3_large">db_mobilenet_v3_large</a></p></li>
</ul>
<p>For a comprehensive comparison, we have compiled a detailed benchmark on publicly available datasets:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 13%" />
<col style="width: 11%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 7%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" colspan="3"></th>
<th class="head" colspan="2"><p>FUNSD</p></th>
<th class="head" colspan="2"><p>CORD</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p><strong>Input shape</strong></p></td>
<td><p><strong># params</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>FPS</strong></p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td><p>25.2 M</p></td>
<td><p>82.14</p></td>
<td><p>87.64</p></td>
<td><p>92.49</p></td>
<td><p>89.66</p></td>
<td><p>2.1</p></td>
</tr>
<tr class="row-even"><td><p>db_mobilenet_v3_large</p></td>
<td><p>(1024, 1024, 3)</p></td>
<td><p>4.2 M</p></td>
<td><p>79.35</p></td>
<td><p>84.03</p></td>
<td><p>81.14</p></td>
<td><p>66.85</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>All text detection models above have been evaluated using both the training and evaluation sets of FUNSD and CORD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both FUNSD subsets combined have 199 pages which might not be representative enough of the model capabilities</em></p>
<p>FPS (Frames per second) is computed after a warmup phase of 100 tensors (where the batch size is 1), by measuring the average number of processed tensors per second over 1000 samples. Those results were obtained on a <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c5/">c5.x12large</a> AWS instance (CPU Xeon Platinum 8275L).</p>
</section>
<section id="detection-predictors">
<h3>Detection predictors<a class="headerlink" href="#detection-predictors" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="models.html#doctr.models.detection.detection_predictor">detection_predictor</a> wraps your detection model to make it easily useable with your favorite deep learning framework seamlessly.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">detection_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictor</span> <span class="o">=</span> <span class="n">detection_predictor</span><span class="p">(</span><span class="s1">&#39;db_resnet50&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_img</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">dummy_img</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="text-recognition">
<h2>Text Recognition<a class="headerlink" href="#text-recognition" title="Permalink to this heading">#</a></h2>
<p>The task consists of transcribing the character sequence in a given image.</p>
<section id="id1">
<h3>Available architectures<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The following architectures are currently supported:</p>
<ul class="simple">
<li><p><a class="reference external" href="models.html#doctr.models.recognition.crnn_vgg16_bn">crnn_vgg16_bn</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.crnn_mobilenet_v3_small">crnn_mobilenet_v3_small</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.crnn_mobilenet_v3_large">crnn_mobilenet_v3_large</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.sar_resnet31">sar_resnet31</a></p></li>
<li><p><a class="reference external" href="models.html#doctr.models.recognition.master">master</a></p></li>
</ul>
<p>For a comprehensive comparison, we have compiled a detailed benchmark on publicly available datasets:</p>
<div class="table-wrapper docutils container" id="id5">
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Text recognition model zoo</span><a class="headerlink" href="#id5" title="Permalink to this table">#</a></caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Input shape</p></th>
<th class="head"><p># params</p></th>
<th class="head"><p>FUNSD</p></th>
<th class="head"><p>CORD</p></th>
<th class="head"><p>FPS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>crnn_vgg16_bn</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>15.8M</p></td>
<td><p>87.15</p></td>
<td><p>92.92</p></td>
<td><p>12.8</p></td>
</tr>
<tr class="row-odd"><td><p>crnn_mobilenet_v3_small</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>2.1M</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>crnn_mobilenet_v3_large</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>4.5M</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>sar_resnet31</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>56.2M</p></td>
<td><p><strong>87.70</strong></p></td>
<td><p><strong>93.41</strong></p></td>
<td><p>2.7</p></td>
</tr>
<tr class="row-even"><td><p>master</p></td>
<td><p>(32, 128, 3)</p></td>
<td><p>67.7M</p></td>
<td><p>87.62</p></td>
<td><p>93.27</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>All text recognition models above have been evaluated using both the training and evaluation sets of FUNSD and CORD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metric being used (exact match) are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p>While most of our recognition models were trained on our french vocab (cf. <a class="reference internal" href="datasets.html#vocabs"><span class="std std-ref">Supported Vocabs</span></a>), you can easily access the vocab of any model as follows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">recognition_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictor</span> <span class="o">=</span> <span class="n">recognition_predictor</span><span class="p">(</span><span class="s1">&#39;crnn_vgg16_bn&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">predictor</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;vocab&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><em>Disclaimer: both FUNSD subsets combine have 30595 word-level crops which might not be representative enough of the model capabilities</em></p>
<p>FPS (Frames per second) is computed after a warmup phase of 100 tensors (where the batch size is 1), by measuring the average number of processed tensors per second over 1000 samples. Those results were obtained on a <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c5/">c5.x12large</a> AWS instance (CPU Xeon Platinum 8275L).</p>
</section>
<section id="recognition-predictors">
<h3>Recognition predictors<a class="headerlink" href="#recognition-predictors" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="models.html#doctr.models.recognition.recognition_predictor">recognition_predictor</a> wraps your recognition model to make it easily useable with your favorite deep learning framework seamlessly.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">recognition_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictor</span> <span class="o">=</span> <span class="n">recognition_predictor</span><span class="p">(</span><span class="s1">&#39;crnn_vgg16_bn&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_img</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">dummy_img</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="end-to-end-ocr">
<h2>End-to-End OCR<a class="headerlink" href="#end-to-end-ocr" title="Permalink to this heading">#</a></h2>
<p>The task consists of both localizing and transcribing textual elements in a given image.</p>
<section id="id3">
<h3>Available architectures<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>You can use any combination of detection and recognition models supporte by DocTR.</p>
<p>For a comprehensive comparison, we have compiled a detailed benchmark on publicly available datasets:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 36%" />
<col style="width: 11%" />
<col style="width: 13%" />
<col style="width: 8%" />
<col style="width: 11%" />
<col style="width: 13%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head" colspan="3"><p>FUNSD</p></th>
<th class="head" colspan="3"><p>CORD</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>FPS</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>FPS</strong></p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + crnn_vgg16_bn</p></td>
<td><p>71.00</p></td>
<td><p>76.02</p></td>
<td><p>0.85</p></td>
<td><p>83.87</p></td>
<td><p>81.34</p></td>
<td><p>1.6</p></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + master</p></td>
<td><p>71.03</p></td>
<td><p>76.06</p></td>
<td></td>
<td><p>84.49</p></td>
<td><p>81.94</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + sar_resnet31</p></td>
<td><p>71.25</p></td>
<td><p>76.29</p></td>
<td><p>0.27</p></td>
<td><p>84.50</p></td>
<td><p><strong>81.96</strong></p></td>
<td><p>0.83</p></td>
</tr>
<tr class="row-even"><td><p>db_mobilenet_v3_large + crnn_vgg16_bn</p></td>
<td><p>67.73</p></td>
<td><p>71.73</p></td>
<td></td>
<td><p>71.65</p></td>
<td><p>59.03</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Gvision text detection</p></td>
<td><p>59.50</p></td>
<td><p>62.50</p></td>
<td></td>
<td><p>75.30</p></td>
<td><p>70.00</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Gvision doc. text detection</p></td>
<td><p>64.00</p></td>
<td><p>53.30</p></td>
<td></td>
<td><p>68.90</p></td>
<td><p>61.10</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AWS textract</p></td>
<td><p><strong>78.10</strong></p></td>
<td><p><strong>83.00</strong></p></td>
<td></td>
<td><p><strong>87.50</strong></p></td>
<td><p>66.00</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>All OCR models above have been evaluated using both the training and evaluation sets of FUNSD and CORD (cf. <a class="reference internal" href="datasets.html#datasets"><span class="std std-ref">Available Datasets</span></a>).
Explanations about the metrics being used are available in <a class="reference internal" href="utils.html#metrics"><span class="std std-ref">Task evaluation</span></a>.</p>
<p><em>Disclaimer: both FUNSD subsets combine have 199 pages which might not be representative enough of the model capabilities</em></p>
<p>FPS (Frames per second) is computed after a warmup phase of 100 tensors (where the batch size is 1), by measuring the average number of processed frames per second over 1000 samples. Those results were obtained on a <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c5/">c5.x12large</a> AWS instance (CPU Xeon Platinum 8275L).</p>
<p>Since you may be looking for specific use cases, we also performed this benchmark on private datasets with various document types below. Unfortunately, we are not able to share those at the moment since they contain sensitive information.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<colgroup>
<col style="width: 30%" />
<col style="width: 8%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head" colspan="2"><p>Receipts</p></th>
<th class="head" colspan="2"><p>Invoices</p></th>
<th class="head" colspan="2"><p>IDs</p></th>
<th class="head" colspan="2"><p>US Tax Forms</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Architecture</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
<td><p><strong>Recall</strong></p></td>
<td><p><strong>Precision</strong></p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + crnn_vgg16_bn (ours)</p></td>
<td><p>78.70</p></td>
<td><p>81.12</p></td>
<td><p>65.80</p></td>
<td><p>70.70</p></td>
<td><p>50.25</p></td>
<td><p>51.78</p></td>
<td><p>79.08</p></td>
<td><p>92.83</p></td>
</tr>
<tr class="row-even"><td><p>db_resnet50 + master (ours)</p></td>
<td><p><strong>79.00</strong></p></td>
<td><p><strong>81.42</strong></p></td>
<td><p>65.57</p></td>
<td><p>69.86</p></td>
<td><p>51.34</p></td>
<td><p>52.90</p></td>
<td><p>78.86</p></td>
<td><p>92.57</p></td>
</tr>
<tr class="row-odd"><td><p>db_resnet50 + sar_resnet31 (ours)</p></td>
<td><p>78.94</p></td>
<td><p>81.37</p></td>
<td><p>65.89</p></td>
<td><p><strong>70.79</strong></p></td>
<td><p><strong>51.78</strong></p></td>
<td><p><strong>53.35</strong></p></td>
<td><p>79.04</p></td>
<td><p>92.78</p></td>
</tr>
<tr class="row-even"><td><p>db_mobilenet_v3_large + crnn_vgg16_bn (ours)</p></td>
<td><p>78.36</p></td>
<td><p>74.93</p></td>
<td><p>63.04</p></td>
<td><p>68.41</p></td>
<td><p>39.36</p></td>
<td><p>41.75</p></td>
<td><p>72.14</p></td>
<td><p>89.97</p></td>
</tr>
<tr class="row-odd"><td><p>Gvision doc. text detection</p></td>
<td><p>68.91</p></td>
<td><p>59.89</p></td>
<td><p>63.20</p></td>
<td><p>52.85</p></td>
<td><p>43.70</p></td>
<td><p>29.21</p></td>
<td><p>69.79</p></td>
<td><p>65.68</p></td>
</tr>
<tr class="row-even"><td><p>AWS textract</p></td>
<td><p>75.77</p></td>
<td><p>77.70</p></td>
<td><p><strong>70.47</strong></p></td>
<td><p>69.13</p></td>
<td><p>46.39</p></td>
<td><p>43.32</p></td>
<td><p><strong>84.31</strong></p></td>
<td><p><strong>98.11</strong></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="two-stage-approaches">
<h3>Two-stage approaches<a class="headerlink" href="#two-stage-approaches" title="Permalink to this heading">#</a></h3>
<p>Those architectures involve one stage of text detection, and one stage of text recognition. The text detection will be used to produces cropped images that will be passed into the text recognition block. Everything is wrapped up with <a class="reference external" href="models.html#doctr.models.ocr_predictor">ocr_predictor</a>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">doctr.models</span> <span class="kn">import</span> <span class="n">ocr_predictor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ocr_predictor</span><span class="p">(</span><span class="s1">&#39;db_resnet50&#39;</span><span class="p">,</span> <span class="s1">&#39;crnn_vgg16_bn&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_page</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">800</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">input_page</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="what-should-i-do-with-the-output">
<h3>What should I do with the output?<a class="headerlink" href="#what-should-i-do-with-the-output" title="Permalink to this heading">#</a></h3>
<p>The ocr_predictor returns a <cite>Document</cite> object with a nested structure (with <cite>Page</cite>, <cite>Block</cite>, <cite>Line</cite>, <cite>Word</cite>, <cite>Artefact</cite>).
To get a better understanding of our document model, check our <a class="reference internal" href="io.html#document-structure"><span class="std std-ref">Document structure</span></a> section</p>
<p>Here is a typical <cite>Document</cite> layout:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">Document</span><span class="p">(</span>
  <span class="p">(</span><span class="n">pages</span><span class="p">):</span> <span class="p">[</span><span class="n">Page</span><span class="p">(</span>
    <span class="n">dimensions</span><span class="o">=</span><span class="p">(</span><span class="mi">340</span><span class="p">,</span> <span class="mi">600</span><span class="p">)</span>
    <span class="p">(</span><span class="n">blocks</span><span class="p">):</span> <span class="p">[</span><span class="n">Block</span><span class="p">(</span>
      <span class="p">(</span><span class="n">lines</span><span class="p">):</span> <span class="p">[</span><span class="n">Line</span><span class="p">(</span>
        <span class="p">(</span><span class="n">words</span><span class="p">):</span> <span class="p">[</span>
          <span class="n">Word</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;No.&#39;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.91</span><span class="p">),</span>
          <span class="n">Word</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;RECEIPT&#39;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.99</span><span class="p">),</span>
          <span class="n">Word</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s1">&#39;DATE&#39;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.96</span><span class="p">),</span>
        <span class="p">]</span>
      <span class="p">)]</span>
      <span class="p">(</span><span class="n">artefacts</span><span class="p">):</span> <span class="p">[]</span>
    <span class="p">)]</span>
  <span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can also export them as a nested dict, more appropriate for JSON format:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">json_output</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">export</span><span class="p">()</span>
</pre></div>
</div>
<p>For reference, here is the JSON export for the same <cite>Document</cite> as above:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s1">&#39;pages&#39;</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
          <span class="s1">&#39;page_idx&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
          <span class="s1">&#39;dimensions&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">340</span><span class="p">,</span> <span class="mi">600</span><span class="p">),</span>
          <span class="s1">&#39;orientation&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
          <span class="s1">&#39;language&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
          <span class="s1">&#39;blocks&#39;</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">{</span>
                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1357421875</span><span class="p">,</span> <span class="mf">0.0361328125</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8564453125</span><span class="p">,</span> <span class="mf">0.8603515625</span><span class="p">)),</span>
                  <span class="s1">&#39;lines&#39;</span><span class="p">:</span> <span class="p">[</span>
                      <span class="p">{</span>
                          <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1357421875</span><span class="p">,</span> <span class="mf">0.0361328125</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8564453125</span><span class="p">,</span> <span class="mf">0.8603515625</span><span class="p">)),</span>
                          <span class="s1">&#39;words&#39;</span><span class="p">:</span> <span class="p">[</span>
                              <span class="p">{</span>
                                  <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;No.&#39;</span><span class="p">,</span>
                                  <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="mf">0.914085328578949</span><span class="p">,</span>
                                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.5478515625</span><span class="p">,</span> <span class="mf">0.06640625</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5810546875</span><span class="p">,</span> <span class="mf">0.0966796875</span><span class="p">))</span>
                              <span class="p">},</span>
                              <span class="p">{</span>
                                  <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;RECEIPT&#39;</span><span class="p">,</span>
                                  <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="mf">0.9949972033500671</span><span class="p">,</span>
                                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1357421875</span><span class="p">,</span> <span class="mf">0.0361328125</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.51171875</span><span class="p">,</span> <span class="mf">0.1630859375</span><span class="p">))</span>
                              <span class="p">},</span>
                              <span class="p">{</span>
                                  <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;DATE&#39;</span><span class="p">,</span>
                                  <span class="s1">&#39;confidence&#39;</span><span class="p">:</span> <span class="mf">0.9578408598899841</span><span class="p">,</span>
                                  <span class="s1">&#39;geometry&#39;</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.1396484375</span><span class="p">,</span> <span class="mf">0.3232421875</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.185546875</span><span class="p">,</span> <span class="mf">0.3515625</span><span class="p">))</span>
                              <span class="p">}</span>
                          <span class="p">]</span>
                      <span class="p">}</span>
                  <span class="p">],</span>
                  <span class="s1">&#39;artefacts&#39;</span><span class="p">:</span> <span class="p">[]</span>
              <span class="p">}</span>
          <span class="p">]</span>
      <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="using_model_export.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Preparing your model for inference</div>
              </div>
              <svg><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="installing.html">
              <svg><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Installation</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2021-2022, Mindee
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Choosing the right model</a><ul>
<li><a class="reference internal" href="#text-detection">Text Detection</a><ul>
<li><a class="reference internal" href="#available-architectures">Available architectures</a></li>
<li><a class="reference internal" href="#detection-predictors">Detection predictors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-recognition">Text Recognition</a><ul>
<li><a class="reference internal" href="#id1">Available architectures</a></li>
<li><a class="reference internal" href="#recognition-predictors">Recognition predictors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#end-to-end-ocr">End-to-End OCR</a><ul>
<li><a class="reference internal" href="#id3">Available architectures</a></li>
<li><a class="reference internal" href="#two-stage-approaches">Two-stage approaches</a></li>
<li><a class="reference internal" href="#what-should-i-do-with-the-output">What should I do with the output?</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/furo.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/js/custom.js"></script>
    </body>
</html>